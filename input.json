{
  "Version": "1",
  "Year": "2024",
  "Semester": "Fall",
  "project_name": "Natural Language Query Interface for Library Data",
  "Objective": " \n            The goal of this project is to develop an advanced natural language query interface for Library of Congress (LOC) \n            data, combining aspects of web scraping, data structuring, and multi-agent Large Language Models (LLMs) with \n            Retrieval-Augmented Generation (RAG).\n\n            The system will:\n\n            1. Implement a comprehensive web scraping and data acquisition pipeline to collect diverse data types from the \n            Library of Congress, including XML documents, catalog records, audio/video content, PDFs, and datasets.\n            2. Develop robust data extraction and transcription processes for audio, video, and PDF content.\n            3. Create a robust data cleaning and structuring pipeline to transform raw scraped data into organized, \n            analysis-ready datasets.\n            4. Create a unified database or index of the collected and structured data.\n            5. Implement a multi-agent LLM system using RAG techniques to enable natural language querying of the collected data.\n            6. Develop a user-friendly interface for submitting queries and displaying results, including source citations.\n            7. Ensure ethical data collection practices, including respect for rate limits and terms of service.\n            8. Incorporate data validation and quality assurance measures to ensure the reliability of collected information.\n            \n            Key Components and Technologies (specific libraries are subject to change):\n            \n            Web Scraping and Data Acquisition:\n            - Utilize libraries such as Requests, BeautifulSoup4, and Scrapy for web scraping\n            - Implement API interactions for rate-limited resources\n            \n            Audio and Video Transcription:\n            - Implement Whisper for transcribing audio content\n            - Utilize video processing libraries (FFMPEG) for extracting audio from video files\n            \n            PDF Scraping:\n            - Use PyPDF2 or pdf2image for PDF parsing and text extraction\n            - Implement OCR (Optical Character Recognition) using libraries like Tesseract for scanned PDFs with no \n            embedded OCR.\n            - Develop methods for preserving document structure and layout information\n            \n            Data Cleaning and Structuring:\n            - Use Pandas and NumPy for data manipulation and cleaning\n            - Implement natural language processing tools like NLTK or spaCy for text processing\n            - Develop custom algorithms for handling domain-specific data formats\n            \n            Database and Indexing:\n            - Utilize vector databases like Faiss or Pinecone for efficient similarity search\n            - Implement document indexing for quick retrieval\n            - Design a flexible schema to accommodate diverse data types\n            \n            Multi-Agent LLM System:\n            - Integrate LLMs such as GPT-4 or LLAMA\n            - Implement a RAG system using frameworks like LangChain or Hugging Face's RAG model\n            - Develop specialized agents for different types of library data (e.g., catalog records, archival\n             descriptions, transcriptions, PDF content)\n             \n             User Interface:\n             - Create a web-based interface using Streamlit\n             \n             Evaluation and Optimization:\n             - Develop comprehensive evaluation metrics for query relevance and accuracy\n             - Implement mechanisms for continuous system improvement based on user feedback\n             \n\n            ",
  "Dataset": "\n            The project will focus on data from the Library of Congress, including but not limited to:\n\n            1. Collection guide (finding aid) XML: From https://findingaids.loc.gov/exist_collections/ead3master/\n            2. Catalog record XML: From https://lccn.loc.gov/[record_number]/marcxml\n            3. Audio, video, textual, and PDF content: Accessible via API from https://www.loc.gov/collections/\n            4. Datasets: From https://www.loc.gov/collections/selected-datasets/\n            5. Web archives: From https://labs.loc.gov/work/experiments/webarchive-datasets/\n            6. Additional datasets: \n            From https://catalog.data.gov/dataset/?q=library+of+congress&_organization_limit=0&organization=library-of-congress\n\n            The project will involve creating a structured database from these diverse sources, which will serve as a \n            contribution to the open-source community. The database will be shared in a structured format for reproducibility \n            and future use by other researchers.\n\n            ",
  "Rationale": "\n            This project addresses several key needs in the field of library science and information retrieval:\n            \n            1. Unified Access: Enables users to query diverse library resources through a single, intuitive interface.\n            2. Natural Language Understanding: Allows users to ask complex questions without needing to understand specific query \n            languages or database structures.\n            3. Cross-Resource Integration: Facilitates the discovery of connections between different types of library resources.\n            4. Improved Accessibility: Makes vast amounts of library data more accessible to researchers and the general public.\n            5. Citation Tracking: Provides accurate source information, crucial for academic and research purposes.\n            6. Scalability: Creates a framework that could potentially be expanded to include data from multiple institutions.\n            7. Open Data Contribution: Produces a structured, open-source database of library data for future research and \n            development.\n\n            ",
  "Approach": "\n            The project will be approached through several key steps:\n\n            1. Data Acquisition and Preprocessing:\n               - Develop web scraping scripts for XML and catalog data\n               - Implement API interaction for audio, video, text, and PDF content\n               - Create a pipeline for downloading and organizing datasets and web archives\n               - Develop audio and video transcription systems\n               \n            2. Data Cleaning and Structuring:\n               - Implement data cleaning algorithms to handle inconsistencies across different data types\n               - Develop a unified data model to represent diverse library resources\n               - Create a pipeline for structuring and indexing the cleaned data\n            \n            3. Database and Index Creation:\n               - Set up a vector database for efficient similarity search\n               - Implement document indexing for quick information retrieval\n               - Develop a system for regular updates and maintenance of the database\n            \n            4. Multi-Agent LLM System Development:\n               - Integrate and fine-tune LLMs for library-specific tasks\n               - Implement RAG techniques to enhance query responses with relevant context\n               - Develop specialized agents for different types of library data\n            \n            5. Natural Language Query Processing:\n               - Create a query understanding module to interpret user questions\n               - Develop a query routing system to direct questions to appropriate agents\n               - Implement a response synthesis module to combine information from multiple sources\n            \n            6. User Interface Development:\n               - Design and implement a web-based frontend for query input and result display\n               - Create visualizations for displaying relationships between different data sources\n               - Implement features for result filtering and sorting\n               \n            7. Citation and Source Tracking:\n               - Develop a system for tracking and displaying the sources of information in responses\n               - Implement proper citation generation for academic use\n            \n            8. Evaluation and Optimization:\n               - Create a set of test queries covering various aspects of library data\n               - Implement evaluation metrics for response relevance, accuracy, and completeness\n               - Develop a feedback mechanism for continuous system improvement\n            \n            9. Documentation and Deployment:\n               - Create comprehensive documentation for the system architecture and usage\n               - Prepare the structured database for open-source release\n               - Develop a deployment strategy, including considerations for scaling and maintenance\n\n            ",
  "Timeline": "\n            This is a rough timeline for the project:\n\n            - (2 Weeks) Data Acquisition and Preprocessing\n            - (2 Weeks) Data Cleaning and Structuring\n            - (2 Weeks) Database and Index Creation\n            - (4 Weeks) Multi-Agent LLM System Development\n            - (3 Weeks) Natural Language Query Processing\n            - (2 Weeks) User Interface Development\n            - (2 Weeks) Citation and Source Tracking\n            - (2 Weeks) Documentation and Deployment\n            - (1 Week) Final Presentation and Project Wrap-up\n            ",
  "Expected Number Students": "\n            This project will be undertaken by 2 students. The complexity and scope of the project allow for effective \n            distribution of tasks between team members, promoting collaborative learning and development across various aspects \n            of data science, natural language processing, and information retrieval.\n\n            ",
  "Possible Issues": "\n            Potential challenges include:\n\n            1. Data Volume and Diversity: Managing and integrating large volumes of diverse data types from different Library of \n            Congress sources.\n            2. API Rate Limits: Developing strategies to work within the constraints of API rate limits for certain data sources.\n            3. Audio and Video Transcription Accuracy: Ensuring high-quality transcriptions of audio and video content, especially \n            for historical or low-quality recordings.\n            4. PDF Extraction Challenges: Dealing with complex layouts, scanned documents, and preserving structural information \n            in PDFs.\n            5. Data Consistency: Maintaining consistency in data structure and quality across different sources and formats.\n            6. Query Interpretation: Accurately interpreting complex, ambiguous, or domain-specific natural language queries.\n            7. Response Accuracy: Ensuring the accuracy and relevance of responses, especially when combining information from \n            multiple sources.\n            8. Performance Optimization: Balancing system responsiveness with the depth and breadth of data searches.\n            9. Scalability: Designing the system to handle potential future expansion to include data from other institutions.\n            10. User Experience: Creating an intuitive interface that caters to users with varying levels of research experience.\n            11. Ethical Considerations: Ensuring proper handling of potentially sensitive or copyrighted information in the \n            Library of Congress data.\n            12. Citation Accuracy: Developing a robust system for tracking and citing sources accurately across diverse data types.\n            13. Evaluation Metrics: Defining comprehensive metrics to assess the system's performance in handling diverse \n            library-related queries.\n            ",
  "Proposed by": "Paul Kelly and Jonathan Schild",
  "Proposed by email": "pjameskelly@gwu.edu, jschild01@gwu.edu",
  "instructor": "Amir Jafari",
  "instructor_email": "ajafari@gmail.com",
  "github_repo": "https://github.com/jschild01/Capstone"
}